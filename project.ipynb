{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "import nltk\n",
    "import xlrd\n",
    "import string\n",
    "import nltk.corpus\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "import nltk.corpus as corpus\n",
    "nltk.download('vader_lexicon')\n",
    "import re\n",
    "\n",
    "stopwords = corpus.stopwords.words(\"english\")\n",
    "\n",
    "import ast \n",
    "from statistics import mean\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(des):\n",
    "    \n",
    "    des = list(des)\n",
    "    \n",
    "    def get_adj_and_adv(text):\n",
    "        \"\"\"\n",
    "        This functionis to firstly tokenize the words and then select\n",
    "        the words that is tagged as adverbe and adjective\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        text_token = word_tokenize(text_lower)\n",
    "        result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "        words = [(word) for word, tag in result_tags if tag in ('JJ','RB')]\n",
    "        \n",
    "        return (words)\n",
    "    \n",
    "    def get_noun(text):\n",
    "        \"\"\"\n",
    "        This functino is to tokenize the words and select the words\n",
    "        that is tagged as noun\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        text_token = word_tokenize(text_lower)\n",
    "        result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "        words = [(word) for word, tag in result_tags if tag in ('NN')]\n",
    "        return (words)\n",
    "    \n",
    "    def nltk_sentiment(sentence):\n",
    "        \"\"\"\n",
    "        This function is to process the sentiment on each tokenized sentences\n",
    "        and then generate a sentiment value for each sentence\n",
    "        \"\"\"\n",
    "    \n",
    "        nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "        score = nltk_sentiment.polarity_scores(sentence)\n",
    "        return score\n",
    "    \n",
    "    sen_tok = [sent_tokenize(des[i]) for i in range(len(des))]\n",
    "\n",
    "    sen_tok_total = [''.join(sen_tok[i]) for i in range(len(sen_tok))]\n",
    "\n",
    "    x = [nltk_sentiment(sen_tok_total[i]) for i in range(len(sen_tok_total))]\n",
    "\n",
    "    x1 = [(list(x[i].items())[-1][1]) for i in range(len(x)) ]\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value(df):\n",
    "    \"\"\"\n",
    "    Handeling any missing value in the dataframe.\n",
    "    \"\"\"\n",
    "    df[\"item_description\"][df['item_description'] == \"No description yet\"] = \"None\"\n",
    "    return df.fillna(\"None\")\n",
    "\n",
    "def split_label(cat):\n",
    "    \"\"\"\n",
    "    This function splits the category into three sub categories.\n",
    "    \"\"\"\n",
    "    cat_split = cat.str.split(\"/\",n = 2,expand = True)\n",
    "    cat_split = cat_split.rename(index = str,columns = {0:'cat1',1:'cat2',2:'cat3'})\n",
    "    cat_split = cat_split.fillna(\"None\")\n",
    "    \n",
    "    return cat_split\n",
    "\n",
    "\n",
    "def variable_process(df):\n",
    "\n",
    "    sub_cats = split_label(df[\"category_name\"])\n",
    "    \n",
    "    columns = (df[[\"shipping\"]].values, df[[\"brand_name\"]].values, df[[\"item_condition_id\"]].values, \n",
    "               sub_cats.values)\n",
    "    columns_names = (\"shipping\", \"brand_name\", \"item_condition\", \"cat1\", \"cat2\", \"cat3\")\n",
    "\n",
    "\n",
    "    return pd.DataFrame(np.concatenate(columns, axis = 1), columns = columns_names)\n",
    "    \n",
    "## !!! encode() only for train \n",
    "def encode(sub_cat_train):\n",
    "    \"\"\"\n",
    "    This function one hot encode category variables for the training set and the testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    global onehotencoder\n",
    "    \n",
    "    onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    one_hot_train = onehotencoder.fit_transform(sub_cat_train.values).toarray()\n",
    "\n",
    "    return one_hot_train\n",
    "\n",
    "def linear_encoder(sub_cat):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    label_cat = le.fit_transform(sub_cat)\n",
    "    return label_cat\n",
    "    \n",
    "\n",
    "def get_length_of_des(des):\n",
    "    #des = list(df['item_description'])\n",
    "    text_token = [word_tokenize(x) for x in des]\n",
    "    length = [len(t) for t in text_token]\n",
    "    \n",
    "    return length\n",
    "\n",
    "def price(df):\n",
    "    price = df['price'].values\n",
    "    return np.log(price+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return: the input for modeling\n",
    "# !!! data_preparation_train() Only for TRAIN DATA\n",
    "\n",
    "def data_preparation_train(df):\n",
    "\n",
    "    df = missing_value(df)\n",
    "    df_label = split_label(df[\"category_name\"])\n",
    "    df1 = variable_process(df)\n",
    "    df2 = encode(df1)\n",
    "\n",
    "    df[\"sent\"] = get_sentiment(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"sent\"].values.reshape(-1,1), axis=1)\n",
    "    \n",
    "    df[\"length\"] = get_length_of_des(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"length\"].values.reshape(-1,1), axis=1)\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "# data_preparation_test only for test \n",
    "\n",
    "def data_preparation_test(df):\n",
    "\n",
    "    df = missing_value(df)\n",
    "    df_label = split_label(df[\"category_name\"])\n",
    "    df1 = variable_process(df)\n",
    "    df2 = onehotencoder.transform(df1).toarray()\n",
    "\n",
    "    df[\"sent\"] = get_sentiment(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"sent\"].values.reshape(-1,1), axis=1)\n",
    "    \n",
    "    df[\"length\"] = get_length_of_des(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"length\"].values.reshape(-1,1), axis=1)\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:7626: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def data_preparation_linear_train(df):\n",
    "    df = missing_value(df)\n",
    "    df_label = split_label(df[\"category_name\"])\n",
    "    df1 = variable_process(df)\n",
    "    df2 = linear_encoder(df1)\n",
    "\n",
    "    df[\"sent\"] = get_sentiment(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"sent\"].values.reshape(-1,1), axis=1)\n",
    "    \n",
    "    df[\"length\"] = get_length_of_des(df[\"item_description\"])\n",
    "    df2 = np.append(df2, df[\"length\"].values.reshape(-1,1), axis=1)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.tsv\", delimiter = \"\\t\", encoding = \"utf-8\", index_col = False)\n",
    "try_df = df.iloc[:8000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(try_df, try_df[\"price\"], test_size=0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:7626: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:7626: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_train = data_preparation_train(X_train)\n",
    "X_test = data_preparation_test(X_test)\n",
    "\n",
    "y_train = np.log(y_train + 1)\n",
    "y_test = np.log(y_test + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Lasso\n",
    "    clf = linear_model.Lasso(alpha=0.1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # KNN\n",
    "    neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "    neigh.fit(X_train, y_train) \n",
    "    \n",
    "    # predictions\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    cl_pred = clf.predict(X_test)\n",
    "    knn_pred = neigh.predict(X_test)\n",
    "\n",
    "    dic = {\n",
    "        \"rf_pred\" : rf_pred,\n",
    "        \"cl_pred\" : cl_pred,\n",
    "        \"knn_pred\": knn_pred,\n",
    "        \"y_test\" : y_test\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(dic)\n",
    "    \n",
    "    df[\"mean\"] = df[[\"rf_pred\", \"cl_pred\", \"knn_pred\"]].mean(axis = 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, actual = y_test):\n",
    "    return np.sqrt(mean_squared_error(actual, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf_pred     0.628738\n",
       "cl_pred     0.740228\n",
       "knn_pred    0.771328\n",
       "mean        0.652519\n",
       "dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[[\"rf_pred\", \"cl_pred\", \"knn_pred\", \"mean\"]].apply(lambda x: mse(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
